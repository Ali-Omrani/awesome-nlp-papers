# Awesome NLP Papers

This is a collection/reading-list of awesome Natural Language Processing papers sorted by date.

### 2018

- [X] **Unsupervised Machine Translation Using Monolingual Corpora Only**, Lample et al.
[`Paper`](https://arxiv.org/abs/1711.00043)

- [X] **On the Dimensionality of Word Embeddings**, Yin et al.
[`Paper`](https://papers.nips.cc/paper/7368-on-the-dimensionality-of-word-embedding)

- [X] **An efficient framework for learning sentence representations**, Logeswaran et al.
[`Paper`](https://arxiv.org/abs/1803.02893)

- [X] **Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation**, Akira Utsumi
[`Paper`](http://aclweb.org/anthology/D18-1520)

- [X] **Domain Adapted Word Embeddings for Improved Sentiment Classification**, Sarma et al.
[`Paper`](https://arxiv.org/abs/1805.04576)

- [X] **In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition**, Sheikhshab et al.
[`Paper`](http://www.aclweb.org/anthology/W18-5618)

- [X] **Generalizing Word Embeddings using Bag of Subwords**, Zhao et al.
[`Paper`](https://arxiv.org/abs/1809.04259)

- [X] **What's in Your Embedding, And How It Predicts Task Performance**, Rogers et al.
[`Paper`](http://www.aclweb.org/anthology/C18-1228)

- [X] **On Learning Better Word Embeddings from Chinese Clinical Records: Study on Combining In-Domain and Out-Domain Data** Wang et al.
[`Paper`](http://www.aclweb.org/anthology/W18-2323)

- [X] **Predicting and interpreting embeddings for out of vocabulary words in downstream tasks**, Garneau et al.
[`Paper`](http://www.aclweb.org/anthology/W18-5439)

- [X] **Addressing Low-Resource Scenarios with Character-aware Embeddings**, Papay et al.
[`Paper`](http://www.aclweb.org/anthology/W18-1204)

- [X] **Domain Adaptation for Disease Phrase Matching with Adversarial Networks**, Liu et al.
[`Paper`](http://www.aclweb.org/anthology/W18-2315)

- [X] **Investigating Effective Parameters for Fine-tuning of Word Embeddings Using Only a Small Corpus**, Komiya et al.
[`Paper`](http://www.aclweb.org/anthology/W18-3408)

- [X] **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**, Devlin et al.
[`Paper`](https://arxiv.org/abs/1810.04805)

- [X] **Adapting Word Embeddings from Multiple Domains to Symptom Recognition from Psychiatric Notes**, Zhang et al.
[`Paper`](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5961810/)

- [ ] **Evaluation of sentence embeddings in downstream and linguistic probing tasks**, Perone et al.
[`Paper`](https://arxiv.org/abs/1806.06259)

- [ ] **Universal Sentence Encoder**, Cer et al.
[`Paper`](https://arxiv.org/abs/1803.11175)

- [X] **Deep Contextualized Word Representations**, Peters et al.
[`Paper`](https://arxiv.org/abs/1802.05365)

- [X] **Learned in Translation: Contextualized Word Vectors**, McCann et al.
[`Paper`](https://arxiv.org/abs/1708.00107)

- [X] **Concatenated p-mean Word Embeddings as Universal Cross-Lingual Sentence Representations**, Rücklé et al.
[`paper`](https://arxiv.org/abs/1803.01400)

- [X] **A Compressed Sensing View of Unsupervised Text Embeddings, Bag-Of-n-Grams, and LSTMs**, Arora et al.
[`Paper`](https://openreview.net/pdf?id=B1e5ef-C-)

### 2017:

- [X] **Attention Is All You Need**, Vaswani et al.
[`Paper`](http://papers.nips.cc/paper/7181-attention-is-all-you-need)

- [X] **Skip-Gram – Zipf + Uniform = Vector Additivity**, Gittens et al.
[`Paper`](http://www.aclweb.org/anthology/P17-1007)

- [X] **A Simple but Tough-to-beat Baseline for Sentence Embeddings**, Arora et al.
[`Paper`](https://openreview.net/pdf?id=SyK00v5xx)

- [X] **Fast and Accurate Entity Recognition with Iterated Dilated Convolutions**, Strubell et al.
[`Paper`](https://arxiv.org/abs/1702.02098)

- [X] **Advances in Pre-Training Distributed Word Representations**, Mikolov et al.
[`Paper`](https://arxiv.org/abs/1712.09405)

- [X] **Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets**, Dror et al.
[`Paper`](https://arxiv.org/abs/1709.09500)

### 2016:

- [X] **Towards Universal Paraphrastic Sentence Embeddings**, Wieting et al.
[`Paper`](https://arxiv.org/abs/1511.08198)

- [X] **Bag of Tricks for Efficient Text Classification**, Joulin et al.
[`Paper`](https://arxiv.org/abs/1607.01759)

- [X] **Enriching Word Vectors with Subword Information**, Bojanowski et al.
[`Paper`](https://arxiv.org/abs/1607.04606)

- [X] **Assessing the Corpus Size vs. Similarity Trade-off for Word Embeddings in Clinical NLP**, Kirk Roberts
[`Paper`](http://www.aclweb.org/anthology/W16-4208)

- [X] **How to Train Good Word Embeddings for Biomedical NLP**, Chiu et al.
[`Paper`](http://www.aclweb.org/anthology/W16-2922)

- [X] **Log-Linear Models, MEMMs, and CRFs**, Michael Collins
[`Paper`](http://www.cs.columbia.edu/~mcollins/crf.pdf)

- [X] **Counter-fitting Word Vectors to Linguistic Constraints**, Mrkšić et al.
[`Paper`](https://arxiv.org/abs/1603.00892)

- [X] **Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation**, Wu et al.
[`Paper`](https://arxiv.org/abs/1609.08144)

### 2015:

- [ ] **Semi-supervised Sequence Learning**, Dai et al.
[`Paper`](https://arxiv.org/abs/1511.01432)

- [X] **Evaluating distributed word representations for capturing semantics of biomedical concepts**, Th et al.
[`Paper`](http://www.aclweb.org/anthology/W15-3820)

### 2014:

- [X] **GloVe: Global Vectors for Word Representation**, Pennington et al.
[`Paper`](https://www.aclweb.org/anthology/D14-1162)

- [X] **Linguistic Regularities in Sparse and Explicit Word Representations**, Levy and Goldberg.
[`Paper`](https://www.cs.bgu.ac.il/~yoavg/publications/conll2014analogies.pdf)

- [X] **Neural Word Embedding as Implicit Matrix Factorization**, Levy and Goldberg.
[`Paper`](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf)

- [X] **word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method**, Goldberg and Levy.
[`Paper`](https://arxiv.org/abs/1402.3722)

- [X] **What’s in a p-value in NLP?**, Søgaard et al.
[`Paper`](http://www.aclweb.org/anthology/W14-1601)

- [X] **How transferable are features in deep neural networks?**, Yosinski et al.
[`Paper`](http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-n%E2%80%A6)

- [X] **Improving lexical embeddings with semantic knowledge**, Yu et al.
[`Paper`](http://www.aclweb.org/anthology/P14-2089)

- [X] **Retrofitting word vectors to semantic lexicons**, Faruqui et al.
[`Paper`](https://arxiv.org/abs/1411.4166)

### 2013:

- [X] **Efficient Estimation of Word Representations in Vector Space**, Mikolov et al.
[`Paper`](https://arxiv.org/pdf/1301.3781.pdf)

- [X] **Linguistic Regularities in Continuous Space Word Representations**, Mikolov et al.
[`Paper`](https://www.aclweb.org/anthology/N13-1090)

- [X] **Distributed Representations of Words and Phrases and their Compositionality**, Mikolov et al.
[`Paper`](https://arxiv.org/abs/1310.4546)

### 2012:

- [X] **An Empirical Investigation of Statistical Significance in NLP**, Berg-Kirkpatrick et al.
[`Paper`](https://dl.acm.org/citation.cfm?id=2391058)

### 2010:

- [X] **Word representations: A simple and general method for semi-supervised learning**, Turian et al.
[`Paper`](https://dl.acm.org/citation.cfm?id=1858721)

### 2008:

- [ ] **A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning**, Collobert and Weston.
[`Paper`](https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf)

### 2006:

- [X] **Domain adaptation with structural correspondence learning**, Blitzer et al.
[`Paper`](https://dl.acm.org/citation.cfm?id=1610094)

### 2003:

- [X] **A Neural Probabilistic Language Model**, Bengio et al.
[`Paper`](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

### 1986:

- [ ] **Distributed Representations**, Hinton et al.
[`Paper`](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf)
